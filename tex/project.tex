\documentclass[a4paper, 12pt]{article}

\usepackage[utf8]{inputenc}
% \usepackage{tgtermes}
% \usepackage{fouriernc}
\usepackage[T1]{fontenc}
\usepackage[margin=3cm]{geometry}
\usepackage{babel}

\usepackage{amssymb}
\usepackage{amsmath}

\usepackage{ebproof}

\usepackage{enumerate}
\usepackage{verbatim}

\usepackage{xcolor}

\usepackage{listings}
\lstset{mathescape=true,
 xleftmargin=.25in}
\usepackage{quiver}

\usepackage[skip=10pt]{parskip}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\B}{\mathbb{B}}
\newcommand{\dif}{\mathrm{d}}
\newcommand{\Prop}{\mathit{Prop}}
\newcommand{\type}{\text{ type}}
\newcommand{\Type}{\mathit{Type}}
\newcommand{\Sort}{\mathit{Sort\;}}
\renewcommand{\Prop}{\mathit{Prop}}
\DeclareMathOperator{\Arg}{Arg}
\newcommand{\la}[1]{\lambda{#1}.\,}


% Kan Danny godt lide
\usepackage[autostyle]{csquotes}
\usepackage{kpfonts}
\usepackage{inconsolata}
\linespread{1.06}

\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{%
	pdftitle=Mathematical project in Lean,
	pdfauthor={Andreas Bøgh Poulsen},
	colorlinks,
	linkcolor={red!50!black},
	citecolor={red!50!black},
	urlcolor={red!50!black},
	bookmarksnumbered=true
}

\usepackage[ntheorem]{mdframed}
\usepackage[amsmath,thmmarks,hyperref]{ntheorem}
\usepackage[capitalize]{cleveref}

% Frame for theorems
\definecolor{shadecolor}{gray}{0.93}
\definecolor{rulecolor}{gray}{0.4}
\mdfdefinestyle{thmframed}{%
	%usetwoside=false, % For use with memoir twoside
	skipabove=0.5em plus 0.4em minus 0.2em,
	skipbelow=0.5em plus 0.4em minus 0.2em,
	leftmargin=-7pt, rightmargin=-7pt, innerleftmargin=6pt,
	innerrightmargin=6pt, innertopmargin=6pt, innerbottommargin=3pt,
	linewidth=1pt, linecolor=rulecolor, backgroundcolor=shadecolor,
	splittopskip=1.2em minus 0.2em,
	splitbottomskip=0.5em plus 0.2em minus 0.1em,
}

% New theorem style with a dot
\makeatletter
\newtheoremstyle{changedot}%
  {\item[\hskip\labelsep \theorem@headerfont ##2~~$\cdot$~~##1\theorem@separator]}%
  {\item[\hskip\labelsep \theorem@headerfont ##2~~$\cdot$~~##1\ (##3)\theorem@separator]}

\newtheoremstyle{changedotbreak}%
  {\item\hbox to \textwidth{\theorem@headerfont ##2~~$\cdot$~~##1\theorem@separator\hfill}}%
  {\item\hbox to \textwidth{\theorem@headerfont ##2~~$\cdot$~~##1\
      (##3)\theorem@separator\hfill}}
\makeatother

\theoremstyle{changedot}
\theoremseparator{.}
\newmdtheoremenv[style=thmframed]{theorem}{Theorem}[section]
\newmdtheoremenv[style=thmframed]{proposition}[theorem]{Proposition}
\newmdtheoremenv[style=thmframed]{lemma}[theorem]{Lemma}
\newmdtheoremenv[style=thmframed]{corollary}[theorem]{Corollary}

\theorembodyfont{\normalfont}
%\theoremsymbol{\ensuremath{\triangle}}
\newmdtheoremenv[style=thmframed]{definition}[theorem]{Definition}

\theoremstyle{changedotbreak}
\newmdtheoremenv[style=thmframed]{definitionbreak}[theorem]{Definition}

\theoremstyle{nonumberplain}
\theoremheaderfont{\normalfont\itshape}
\theorembodyfont{\normalfont}
\theoremsymbol{\ensuremath{\square}}
\newtheorem{proof}{Proof}

\Crefname{theorem}{Theorem}{Theorems}
\Crefname{proposition}{Proposition}{Propositions}
\Crefname{lemma}{Lemma}{Lemmata}
\Crefname{corollary}{Corollary}{Corollaries}
\Crefname{definition}{Definition}{Definitions}

\crefformat{equation}{(#2#1#3)}

% / Kan Danny godt lide

\title{Mathematical project in \textsf{L$\exists \forall$N}\\{\large \textsc{An innocent mathematicians guide to lean}}}
\author{Andreas Bøgh Poulsen, studienummer: 201805425}

\begin{document}
\maketitle
\tableofcontents

\newpage
\begin{abstract}

\end{abstract}
\newpage

\section*{Introduction}
The following is a project, in which I try to learn how to do formalized mathematics, using Lean as my proof checker. This document is a report on my learnings, and is intended as a resource for other mathematicians, who may wish to learn about Lean. 

\section{Martin-Löf dependent type theory}
Dependent type theory is a logical theory, comparable to first order logic. Similarly to how we usually think we do mathematics in first order logic with ZFC set theory on top, we can translate our mathematical theories into other logical theories. In this chapter I'll give a taste of how dependent type theory works as a formal system. If you're only interested in learning Lean, feel free to skip this section.

\subsection{Inference rules}
A Inference rule is on the form

\begin{prooftree}
  \hypo{\Gamma \vdash P}
  \hypo{\Gamma \vdash Q}
  \infer2[$\land$-intro]{\Gamma \vdash P \land Q}
\end{prooftree}

which is read as follows: if we, in a context $\Gamma$, can prove $P$ and in the same environment can prove $Q$, then we can prove $P \land Q$ in the context $\Gamma$.

The defining feature of {\emph type} theory is, that every element has a type. Thus the above is meaningless, as $P$ and $Q$ have no type. Compare this to ZFC, where everything is either a proposition from first order logic, or a set. This leads to weird statements like $0 \in 1$, which is well-posed since everything is a set, but does not carry a meaning in our ``usual'' way of doing mathematics. Type theory asks that every element has a type. This is particularly helpful when doing computerized proofs, as it helps the proof-checker catch weird statements like $0 \in 1$. Since $1$ has the type of a natural number and not the type of a set, Lean can give an error, instead of silently trying to prove what may well have been a typo.

In type theory the above rewrite rule would look like this:

\begin{prooftree}
  % \hypo{\Gamma \vdash \mathit{Prop} \text{ type}}
  \hypo{\Gamma \vdash P : \mathit{Prop}}
  \hypo{\Gamma \vdash Q : \mathit{Prop}}
  \infer2[$\land$-intro]{\Gamma \vdash P \land Q : \Prop}
\end{prooftree}

Everything is read the same, except $P : \Prop$ is read ``$P$ has type $\Prop$''. $\Prop$ is the type of propositions. I will not spend too much time going through every single single inference rule. I will, however, introduce the defining features of dependent type theory: dependent types, and show how they are used.

\begin{definition}
  Type theory has four different \textit{judgements}.
  \begin{enumerate}
    \item $\Gamma \vdash A \text{ type}$ says $A$ is a well-formed type in context $\Gamma$.
    \item $\Gamma \vdash A \doteq B \text{ type}$ says $A$ and $B$ are judgementally equal types in context $\Gamma$.
    \item $\Gamma \vdash a : A$ says $a$ is an element of type $A$ in context $\Gamma$.
      \item $\Gamma \vdash a \doteq b \text{ type}$ says $a$ and $b$ both have type $A$ and are judgementally equal.
  \end{enumerate}
\end{definition}

As we would expect, there are axioms making this an equivalence relation:

\begin{align*}
  &\begin{prooftree}
    \hypo{\Gamma \vdash a : A}
    \infer1{\Gamma \vdash a \doteq a : A}
  \end{prooftree}
  &
  \begin{prooftree}
    \hypo{\Gamma \vdash a \doteq b : A}
    \infer1{\Gamma \vdash b \doteq a : A}
  \end{prooftree}
  &
  &\begin{prooftree}
    \hypo{\Gamma \vdash a \doteq b : A}
    \hypo{\Gamma \vdash b \doteq c : A}
    \infer2{\Gamma \vdash a \doteq c : A}
  \end{prooftree}
\end{align*}

and similarly for types. There is also a rule stating that you can substitute judgementally equal elements anywhere.

Judgemental equality is actually a very strong equality, and many objects we usually consider equal, cannot be proven judgementally equal. Later we'll introduce a weaker equality, that captures better our usual understanding of equality. Stay tuned, the formulation may surprise you.

We need to introduce dependent types as well as functions, before we can get going.

\begin{definition}
  A \textit{dependent type} is a type of the form $\Gamma, x : A \vdash B(x) \text{ type}$ with a rule letting us assume elements of that type:

  \begin{center}
    \begin{prooftree}
      \hypo{\Gamma \vdash A \text{ type}}
      \infer1{\Gamma a : A \vdash a : A}
    \end{prooftree}
  \end{center}


  A \textit{section} of a dependent type $B(x)$ is an element $\Gamma, x : A \vdash b(x) : B(x)$.
\end{definition}

Note that for different $x : A$ in the context, $B(x)$ may be different type. Using dependent types we can introduce functions:

\begin{definition}
  A \textit{function type} is the type of sections of a dependent type $B(x)$, given by the following introduction rules:

  \begin{align*}
    &\begin{prooftree}
      \hypo{\Gamma, x : A \vdash B(x) \text{ type}}
      \infer1{\Gamma \vdash \Pi_{x:A} B(x) \text{ type}}
    \end{prooftree}
      &
    &\begin{prooftree}
      \hypo{\Gamma, x : A \vdash b(x) : B(x)}
      \infer1{\Gamma \vdash \la x b(x) : \Pi_{x:A} B(x)}
    \end{prooftree}
  \end{align*}

  and has the following evaluation rules:

  \begin{align*}
    &\begin{prooftree}
      \hypo{\Gamma \vdash f : \Pi_{x:A} B(x)}
      \infer1{\Gamma, x : A \vdash f(x) : B(x)}
    \end{prooftree}
    &
    &\begin{prooftree}
      \hypo{\Gamma, x : A \vdash b(x) : B(x)}
      \infer1{\Gamma, x : A \vdash (\la x b(x))(x) \doteq b(x) : B(x)}
    \end{prooftree}
  \end{align*}

\end{definition}

\textit{Remark} Not all types are dependent. If $B(x)$ is independent of $x$ we will just write $B$ and functions as $A \to B$. This arrow binds stronger that $\Pi$, so that $\Pi_{a:A}\, B \to C$ is read as $\Pi_{a:A}\, (A \to B)$.

\subsection{Logic in type theory}
We now have the building blocks to start formulating usual logic in dependent type theory. The basic idea is to interpret types as propositions. A proof of a proposition corresponds to an element of a type. Thus a false proposition is a type without any elements, and a true proposition is a type with at least one element. We can introduce canonical false and true propositions:

\begin{definition}
  The types of false and true.

  The empty type (false) is given by
\begin{align*}
  \begin{prooftree}
    \infer0{\vdash \emptyset}
  \end{prooftree} & &
  \begin{prooftree}
    \infer0{\vdash ind_{\emptyset} : \Pi_{x:\emptyset} P(x)}
  \end{prooftree}
\end{align*}

and the unit type (true) is given by
\begin{align*}
  \begin{prooftree}
    \infer0{\vdash \mathbf{1} \type}
  \end{prooftree} & &
                    \begin{prooftree}
                      \infer0{\vdash \bullet : \mathbf 1}
                      \end{prooftree}& &
                      \begin{prooftree}
                        \infer0{\vdash ind_{\mathbf 1} : P(\bullet) \to \Pi_{x : \mathbf 1}\, P(x)}
                      \end{prooftree}
  \end{align*}

\end{definition}

So $\emptyset$ is a false proposition, and $\mathbf 1$ is a true proposition, with the proof $\bullet : \mathbf 1$. What would other logical operators look like in this interpretation? Implication simply becomes a function. $f : A \to B$ says ``f takes an element of $A$ and produces an element of $B$'' or as propositions ``$f$ takes a proof of $A$ and produces a proof of $B$'', which is exactly what an implication does.

In this light, the induction rule of $\emptyset$ states, that given a proof of false, we can prove everything about that element. In particular, $P(x)$ doesn't have to depend on $x$, så given a proof of false, we can prove anything! The induction principle for $\mathbf 1$ is comparatively boring, stating that if something is true about $\bullet$, then it's true about every element of $\mathbf 1$. I other words: if something is true assuming true, and we have a proof of true, that something is true.

We can interpret something being false $\neg A$ as the type $A \to \emptyset$. Then ``$A$ is false'' translates to ``assuming $A$, I can prove false''. We can then prove the statement $(A \implies B) \implies (\neg B \implies \neg A)$. In type theory, this translates to $(A \to B) \to ((B \to \emptyset) \to (A \to \emptyset))$. The construction is as follows:

\begin{theorem}
  $(A \implies B) \implies (\neg B \implies \neg B)$
\end{theorem}
\begin{proof}
  We construct an element of the desired type:
\begin{center}
  \begin{prooftree}
    \hypo{\Gamma \vdash A \text{ type}}
    \infer1{\Gamma, a : A \vdash a : A}
    \hypo{\Gamma \vdash A \text{ type}}
    \infer1{\Gamma, a : A \vdash a : A}
    \hypo{\Gamma \vdash B \text{ type}}
    \infer2{\Gamma \vdash A \to B \text{ type}}
    \infer1{\Gamma, h : A \to B \vdash h : A \to B}
    \infer2{\Gamma, a : A, h : A \to B \vdash h(a) : B}
    \hypo{\Gamma \vdash B \text{ type}}
    \infer1{\Gamma, b : B \vdash b : B}
    \infer0{\vdash \emptyset \text{ type}}
    \infer2{\Gamma \vdash B \to \emptyset \text{ type}}
    \infer1{\Gamma, f : B \to \emptyset \vdash f : B \to \emptyset}
    \infer2{\Gamma, a : A, f : B \to \emptyset, h : A \to B \vdash f(h(a)) : \emptyset}
    \infer1{\Gamma, f : B \to \emptyset, h : A \to B \vdash \la a f(h(a)) : A \to \emptyset}
    \infer1{\Gamma, h : A \to B \vdash \la f \la a f(h(a)) : (B \to \emptyset) \to (A \to \emptyset)}
    \infer1{\Gamma \vdash \la h \la f \la a f(h(a)) : (A \to B) \to ((B \to \emptyset) \to (A \to \emptyset))}
  \end{prooftree}
\end{center}
~
\end{proof}

You'll note that we didn't use $\mathit{ind}_{\emptyset}$ in the constrution. Indeed, this is a special case of the more general formula $(A \to B) \to ((B \to C) \to (A \to C))$, which we get simply by composing functions. We'll denote $f \circ g := \la x f(g(x))$ and refer to the above proof tree for its construction.

So how do we actually use the induction principle $\mathit{ind}_{\emptyset}$? Well, we can't prove much right now, but if we introduce \textit{or}:

\begin{definition}
  The type of disjunction
\begin{align*}
  \begin{prooftree}
    \hypo{\Gamma \vdash A \text{ type}}
    \hypo{\Gamma \vdash B \text{ type}}
    \infer2{\Gamma \vdash A \lor B \text{ type}}
  \end{prooftree} & &
  \begin{prooftree}
    \hypo{\Gamma \vdash a : A}
    \hypo{\Gamma \vdash B \text{ type}}
    \infer2{\Gamma \vdash \iota_{1} : A \to A \lor B}
  \end{prooftree}
  &&
  \begin{prooftree}
    \hypo{\Gamma \vdash A \text{ type}}
    \hypo{\Gamma \vdash b : B}
    \infer2{\Gamma \vdash \iota_{2} : B \to A \lor B}
  \end{prooftree}
\end{align*}
\begin{align*}
  \begin{prooftree}
    \hypo{\Gamma \vdash A \text{ type}}
    \hypo{\Gamma \vdash B \text{ type}}
    \infer2{\Gamma \vdash \mathit{ind}_{\lor} : (\Pi_{a:A} P(\iota_{1}(a))) \to (\Pi_{b:B} P(\iota_{2}(b))) \to (\Pi_{z:A\lor B} P(z))}
    % \hypo{\Gamma \vdash l : \Pi_{a:A}P(a)}
    % \hypo{\Gamma \vdash r : \Pi_{b:B} P(b)}
    % \infer2{\Gamma \vdash ind_{\lor} : \Pi_{z : A \lor B} P(z)}
  \end{prooftree}
  \\ \\
  \begin{prooftree}
    \hypo{\Gamma \vdash a : A}
    \hypo{\Gamma \vdash l : \Pi_{a:A}P(a)}
    \hypo{\Gamma \vdash r : \Pi_{b:B} P(b)}
    \infer3{\Gamma \vdash ind_{\lor}(l, r, \iota_{1}(a)) \doteq l(a) : P(a)}
  \end{prooftree} \\ \\
  \begin{prooftree}
    \hypo{\Gamma \vdash b : B}
    \hypo{\Gamma \vdash l : \Pi_{a:A}P(a)}
    \hypo{\Gamma \vdash r : \Pi_{b:B} P(b)}
    \infer3{\Gamma \vdash ind_{\lor}(l, r, \iota_{2}(b)) \doteq r(b) : P(b)}
  \end{prooftree}
\end{align*}
\end{definition}

we can prove the following: $\neg A \to (A \lor B) \to B$.

\begin{center}
  \begin{prooftree}
    \hypo{\Gamma \vdash A \type}
    \hypo{\Gamma \vdash B \type}
    \infer2{\Gamma \vdash \la h \la z ind_\lor(\mathit{ind_\emptyset} \circ h, \mathit{id}, z) : (A \to \emptyset) \to (A \lor B) \to B}
  \end{prooftree}
\end{center}

Okay, that was quite a mouthful. Let's work through the rules for $\lor$ in order. First, assuming two types $A$ and $B$, we can form the disjunction $A \lor B$. We have two rules for forming elements of $A \lor B$, namely $\iota_{1}$ and $\iota_{2}$ which take an element of $A$, resp. $B$ and forms an element of $A \lor B$. Next line, we have a way to use a disjunction. Given a proof of $P$ assuming $A$ and a proof of $P$ assuming $B$, we get  proof of $P$ assuming $A \lor B$. The final two lines state, that $\mathit{ind}_{\lor}$ behaves the way we expect it to.

Using these, the proof if the assertion above becomes

\begin{theorem}
  $\neg A \implies (A \lor B) \implies B$
\end{theorem}
\begin{proof}
  We construct an element of the desired type:
\begin{center}
  \begin{prooftree}
    \hypo{\Gamma \vdash A, B \type}
    \infer1{\Gamma \vdash A \lor B \type }
    \infer1{\Gamma, z : A \lor B \vdash z : A \lor B}
    \hypo{\Gamma \vdash A \type}
    \infer0{\vdash \emptyset \type}
    \infer2{\Gamma \vdash A \to \emptyset \type}
    \infer1{\Gamma, h : A \to \emptyset \vdash h}
    \infer0{\vdash ind_\emptyset : \dots }
    \infer2{\Gamma \vdash ind_\emptyset \circ h : A \to B}
    \hypo{\Gamma \vdash A, B \type}
    \infer1{\Gamma \vdash ind_\lor : \dots }
    \infer3{\Gamma, h : A \to \emptyset, z : A \lor B \vdash ind_\lor(\mathit{ind_\emptyset} \circ h, \mathit{id}, z) : B}
    \infer1{\Gamma, h : A \to \emptyset \vdash \la z ind_\lor(\mathit{ind_\emptyset} \circ h, \mathit{id}, z) : (A \lor B) \to B}
    \infer1{\Gamma \vdash \la h \la z ind_\lor(\mathit{ind_\emptyset} \circ h, \mathit{id}, z) : (A \to \emptyset) \to (A \lor B) \to B}
  \end{prooftree}
\end{center}
~
\end{proof}

I have omitted some types to make the tree fit the page, but the crux of the argument is, that from an implication $A \to \emptyset$ and a proof of $A$, we can use $ind_{\emptyset}$ to prove $B$. Thus we derive a function $A \to B$, which we can use, together with $id : B \to B$ to prove $B$ from a $A \lor B$.

Okay, so we have negation, implication and disjunction. I encourage you to imagine how conjunction would be defined. But what about quantors? We'll postpone the existential quantor until later, as it's formulation is quite subtle, but universal quantification is surprisingly straightforward. $\forall x. P(x)$ states that for every x $x$, we get a proof of $P(x)$. That sounds like a function to me. And indeed, we simply define $\forall := \Pi$. Thus, implication is a non-dependent function, while universal quantification is a dependent function.

This may be surprising, but it actually highlights a strength of dependent type theory as a logical framework: everything, even proofs, is just elements of types. The disjunction, as defined above, is also known as the coproduct in functional programming languages. In the next section, we'll take full advantage of this idea.

\subsection{The natural numbers}
So far we've only thought about propositions. Let's introduce to natural numbers, as an example of something non-propositional.

\begin{definition}
  The natural numbers

\begin{align*}
 &\begin{prooftree}
    \infer0{\vdash \N \text{ type }}
  \end{prooftree}
   &
  &\begin{prooftree}
    \infer0{\vdash 0_{\N} : \N}
    \end{prooftree}
    &
    &\begin{prooftree}
      \infer0{\vdash succ_{\N} : \N \to \N}
    \end{prooftree}
\end{align*}
\begin{align*}
  \begin{prooftree}
    \hypo{\Gamma, n : \N \vdash P(n) \text{ type}}
    \hypo{\Gamma \vdash p_{0} : P(0_{\N})}
    \hypo{\Gamma \vdash p_{s} : \Pi_{n:\N} (P(n) \to P(succ_{\N}(n)))}
    \infer3{\Gamma \vdash ind_{\N}(p_{0}, p_{s}) : \Pi_{n:\N} P(n)}
  \end{prooftree}
\end{align*}
\begin{align*}
  \begin{prooftree}
      \hypo{\Gamma, n : \N \vdash P(n) \text{ type}}
      \hypo{\Gamma \vdash p_{0} : P(0_{\N})}
      \hypo{\Gamma \vdash p_{s} : \Pi_{n:\N} (P(n) \to P(succ_{\N}(n)))}
      \infer3{\Gamma \vdash ind_{\N}(p_{0}, p_{s}, 0_{\N}) \doteq p_{0} : P(0_{\N})}
  \end{prooftree}
\end{align*}
\begin{align*}
  \begin{prooftree}
      \hypo{\Gamma, n : \N \vdash P(n) \text{ type}}
      \hypo{\Gamma \vdash p_{0} : P(0_{\N})}
      \hypo{\Gamma \vdash p_{s} : \Pi_{n:\N} (P(n) \to P(succ_{\N}(n)))}
      \infer3{\Gamma \vdash ind_{\N}(p_{0}, p_{s}, succ_{\N}(n)) \doteq p_{s}(n, ind_{\N}(p_{0}, p_{s}, n)) : P(succ_{\N}(n))}
  \end{prooftree}
\end{align*}
\end{definition}

The first three rules govern the construction of natural numbers, and the next rule is the induction rule. If we for a moment assume $P$ is a predicate, it reads ``Given a predicate $P$, a proof of $P(0)$ and proof of $P(n) \implies P(succ(n))$ we get a proof of $\forall n: P(n)$.'' The two final rules simply state, that induction behaves as we expect.

All these inference rules are quite heavy. Let's introduce some lighter notation:

\begin{lstlisting}[mathescape=true]
  type $\vdash$ $\N$ :=
  | 0$_\N$ : $\N$
  | succ$_\N$ : $\N \to \N$
\end{lstlisting}

Everything in the inference rules is derivable from this definition. I particular the induction principle becomes \[ind_{\N} : P(0_{\N}) \to (\Pi_{n:\N}\, (P(n) \to P(succ_{\N}(n)))) \to \Pi_{n:\N}\, P(n)\] We can similarly define

\begin{lstlisting}[mathescape=true]
  type $\vdash$ A$\lor$B :=
  | $\iota_{1}$ : A $\to$ A$\lor$B
  | $\iota_{2}$ : B $\to$ A$\lor$B
\end{lstlisting}

We can also observe, that the proof trees so far can be automatically generated, since we every construct so far is introduced by exactly one inference rule. Thus, for the proof of $(A \to \emptyset) \to (A \lor B) \to B$ we'll just write $\la h \la z ind_{\lor}(ind_{\emptyset} \circ h, id, z)$ as the proof, and omit the proof tree.

Let's define addition and prove some identities. We would like addition to respect the following specification:
\begin{align*}
  \mathit{add}_{\N} (0, n) &\doteq n \\
  \mathit{add}_{\N} (succ_{\N}(m), n) &\doteq succ(\mathit{add}_{\N}(m, n))
\end{align*}
and we would like to do it using the induction rule on $\N$. Remember $ind_{\N}(p_{0}, p_{s})$ has type $\Pi_{n:\N} P(n)$ and addition needs to have type $\N \to (\N \to \N)$. Thus $P(n)$ needs to be the type $\N \to \N$. The idea is that $ind_{\N}(p_{0}, p_{s}, n)$ should produce a function adding $n$ to a number. Then $ind_{\N}(p_{0}, p_{s}, n)(m)$ computes $n+m$.

First, let's define $p_{0} := id : \N \to \N$. This is a function taking a number and adding 0 to it. Then we need to define $p_{s} : \N \to (\N \to \N) \to (\N \to \N)$, that is, given a number $n$ and a function adding $n$ to a number, return a function adding $n+1$ to a number. This is simply $p_{s}(n, f) := succ_{\N} \circ f$. Thus

\begin{definition}
  Addition on the natural numbers
  \[add_{\N} := \la m \la n ind_{\N}(id, \la x \la f succ_{\N} \circ f, m)(n) : \N \to \N \to \N.\]
\end{definition}

We can see that it satisfies our specification:
\begin{align*}
  add_{\N}(0_{\N}, n) &\doteq (\la m \la n ind_{\N}(id, \la x \la f succ_{\N} \circ f, m)(n)(0_{\N}, n) \\
  &\doteq ind_{\N}(id, \la x \la f succ_{\N} \circ f, 0_{\N})(n) \\
                 &\doteq id(n) \\
                 &\doteq n \\ \\
  add_{\N}(succ_{\N}(m), n) &\doteq ind_{\N}(id, \la x \la f succ_{\N} \circ f, succ_{\N}(m))(n) \\
                 &\doteq (\la x \la f succ_{\N} \circ f)(n, ind_{\N}(id, \la x \la f succ_{\N} \circ f, m))(n) \\
                 &\doteq (succ_{\N} \circ ind_{\N}(id, \la x \la f succ_{\N} \circ f), m)(n) \\
                 &\doteq succ(ind_{\N}(id, \la x \la f succ_{\N} \circ f), m)(n)) \\
  &\doteq succ(add_{\N}(m, n))
\end{align*}

We can check that $1+2=3$:
\begin{align*}
  add_{\N}(&succ_{\N}(0_{\N}), succ_{\N}(succ_{\N}(0_{\N}))) \\
           &\doteq ind_{\N}(id, \la x \la f succ_{\N} \circ f, succ_{\N}(0_{\N}))(succ_{\N}(succ_{\N}(0_{\N})))) \\
  &\doteq (succ_{\N}\circ ind_{\N}(id, \la x \la f succ_{\N} \circ f, 0_{\N}))(succ_{\N}(succ_{\N}(0_{\N}))) \\
           &\doteq (succ_{\N} \circ id)(succ_{\N}(succ_{\N}(0_{\N}))) \\
  &\doteq succ_{\N}(succ_{\N}(succ_{\N}(0_{\N})))
\end{align*}

\subsection{Equality}
We have seen how the induction principle on types can help us to both prove propositions about them (as we did with $\lor$), and define functions on them (as we did with $\N$). However, there are a couple of notable propsitions about $\N$, which we can't show. Notably, that $\neg(succ_{\N} \doteq 0)$ and that $add_{\N}(n, m) \doteq add_{\N}(m, n)$. The first we can't show, because we have no way of negating a judgement. $A \doteq B$ is not a type, so $\neg (A \doteq B)$ isn't well-formed. The other, we can show for any given $n, m$, but not in general. This is because we need to prove it by induction, but we can't pass an assumption of $n\doteq m$ along to the induction step, since it isn't a type.

To get past both of these problem, we introduce a type of equality:
\begin{definition}
  The type of equality is given by
\begin{lstlisting}[mathescape=true]
  type (a b : A) $\vdash$ $a=_{A}b$ :=
  | refl : $\Pi_{x:A}$ x$=_{A}$x
\end{lstlisting}
  with derived induction principle  
\begin{lstlisting}
ind$_{=_{A}}$ :  $\Pi_{a:A}\, (P(a) \to \Pi_{b:A}\, (a=_{A}b \to P(b)))$
\end{lstlisting}
\end{definition}

This states that for any two elements $a, b : A$, we have the type corresponding to the proposition ``a equals b''. It also states that for any $x : A$, there is an element of type $x=_{A}x$. Note that we can only compare elements of the same type. The induction principle states: ``given $a:A$, a proof/element of $P(a)$, a $b:A$ and a proof of $a=_{A}b$, we obtain a proof/element of $P(b)$.''

It's remarkable that there is no axioms about transitivity or symmetry. These can in fact be derived from the induction principle.
\begin{theorem}
  Equality is transitive, i.e. there is a function
  \[trans_{=_{A}} : \Pi_{a, b, c : A} (a=_{A}b) \to (b=_{A}c) \to (a=_{A}c) \]
\end{theorem}
\begin{proof}
  \[trans_{=_{A}}(a, b, c) := \la{h_{1}} \la{h_{2}} ind_{=_{A}}(b, h_{1}, c, h_{2})\]
\end{proof}
Short and sweet, although the lack of type annotations makes it a little hard to decipher. It might help if we specialize the type of $ind_{=_{A}}$. In our case $P(x)$ means $a=_{A}x$:
\begin{align*}
  ind_{=_{A}} &: \Pi_{a:A}\, (P(a)& &\to& \Pi_{b:A}\, a&=_{A}b &\to& &P(b)) \\
  ind_{=_{A}} &: \Pi_{b:A}\, (a=_{A}b& &\to& \Pi_{c:A}\, b&=_{A}c &\to& &(a=_{A}c)).
\end{align*}

Similarly, symmetry is just
\begin{theorem}
  Equality is symmetric, i.e. there is a function
  \[symm_{=_{A}} : \Pi_{a, b : A} (a =_{A} b) \to (b =_{A} a) \]
\end{theorem}
\begin{proof}
  \[symm_{=_{A}}(a, b) := \la h ind_{=_{A}}(a, refl(a), b, h)\]
\end{proof}

We can even prove that equality is preserved by functions:
\begin{theorem}
  Function application preserves equality, i.e. there is a function
  \[\mathit{fun\_eq} : \Pi_{a, b: A}\, \Pi_{f : A \to B} (a =_{A} b) \to (f(a) =_{B} f(b))\]
\end{theorem}
\begin{proof}
  \[\mathit{fun\_eq}(a, b, f) := \la h ind_{=_{B}}(a, refl(f(a)), b, h)\]
\end{proof}

We can use this equality to prove the things I mentioned earlier: $0_{\N} \neq succ_{\N}(n)$ and $add_{\N}(n, m) = add_{\N}(m, n)$. But first, let's clean up our notation even more. We'll omit the subscript indicating type, whenever the type is clear from context. Also, instead of using the $ind$ function all the time, we can simply define our functions on each constructor. For example we could define
\begin{definition} Addition by pattern matching
\begin{lstlisting}[mathescape=true]
  def add : $\N \to \N \to \N$
  | 0      , n := n
  | succ(m), n := succ$_{\N}$(add(m, n))
\end{lstlisting}
\end{definition}

and have the specification mechanically translated to functions for $ind_{\N}$. With this, let's prove commutativity. First, we have $add(0, n) \doteq n$ and we get $add(n, 0) = n$ by induction:

\begin{lemma}
  $add(n, 0) = n \doteq add(0, n)$
\end{lemma}
\begin{proof} ~
\begin{lstlisting}[mathescape=true]
  def add_zero : $\Pi_{n:\N}\, add(n, 0) = n$
  | 0       := refl
  | succ(n) := fun_eq(add(n, 0), n, succ, add_zero(n))
\end{lstlisting}~
\end{proof}

Note that the last line proves $add(succ(n), 0) = succ(add(n, 0)) = succ(n)$, which is exactly the induction step. Thus we have $add(n, 0) = add(0, n)$. For the induction step, we need another lemma. We have $add(succ(m), n) \doteq succ(add(m, n))$, and by induction we get:

\begin{lemma}
  $add(m, succ(n)) = succ(add(m, n)) \doteq add(succ(m), n)$
\end{lemma}
\begin{proof}~
\begin{lstlisting}[mathescape=true]
  def succ_add : $\Pi_{m, n : \N}\, add(m, succ(n)) = succ(add(m, n))$
  | 0      , n := refl,
  | succ(m), n := fun_eq( add(m, succ(n))
                        , succ(add(m, n))
                        , succ
                        , succ_add)
\end{lstlisting}~
\end{proof}

Since $add(succ(m), n) \doteq succ(add(m, n))$, this lemma states that $add(succ(m), n) = add(m, succ(n))$. Together, we have
\begin{theorem}
  Addition is commutative
\end{theorem}
\begin{proof}~
\begin{lstlisting}[mathescape=true]
def add_comm : $\Pi_{m, n : \N}\, add(m, n) = add(n, m)$
| 0      , n := symm(add(n, 0), add(0, n), add_zero(n))
| succ(m), n := trans( succ(add(m, n))
                     , succ(add(n, m))
                     , add(n, succ(m))
                     , fun_eq(add(m, n)
                             , add(n, m)
                             , add_comm(m, n))
                     , symm( add(n, succ(m))
                           , succ(add(n, m))
                           , succ_add(n, m) )
                     )
\end{lstlisting}~
\end{proof}

\subsection{Higher order types}
I promised to prove $0 \neq succ(n)$, but this is surprisingly hard. At least, it requires a little bit more machinery. Namely, the concept of higher order types, or functions that produces types. We have actually already seen them, $=_{A}$ is example of a higher order type. We can see $=_{A}$ as a function $A \to A \to \mathit{Type}$, where $\mathit{Type}$ is the type of types. Does that even make sense? We'll talk more about it in the next section, but for now, we'll just assume that every type is itself an element of type $\mathit{Type}$.

This enables us to produce functions such as

\begin{lstlisting}[mathescape=true]
def nat_equals : $\N \to \N \to \Type$
| 0      , 0       := $\mathbf 1$
| 0      , succ(n) := $\emptyset$
| succ(n), 0       := $\emptyset$
| succ(n), succ(m) := nat_equals(n, m)
\end{lstlisting}

which is exactly what we need. Remember, that $0 \neq succ(n) \doteq \neg(0 = succ(n)) \doteq (0 = succ(n)) \to \emptyset$. Thus we have:

\begin{theorem}\label{thm:zero_ne_succ}
  $0$ is the first element of $\N$, i.e. we have $\Pi_{n:\N} \neg(0 = succ(n))$.
\end{theorem}
\begin{proof}~
  We interpret \lstinline{$\bullet$ : nat_equals(0, 0)}, since \lstinline{nat_equals(0, 0) $\doteq$ $\mathbf 1$}. Then, using the assumption $0 = succ(n)$, we rewrite the last $0$ in that type, to obtain an element of type \lstinline{nat_equals(0, succ(n))}.

\begin{lstlisting}[mathescape=true]
def zero_ne_succ : $\Pi_{n : \N} (0 = succ(n)) \to nat\_equals(0, succ(n))$
| n, h := ind$_{=}$(0, $\bullet$, succ(n), h)
\end{lstlisting}
  And this is our desired function, since \lstinline{nat_equals(0, succ(n)) $\doteq\;$ $\emptyset$}.
\end{proof}

\subsection{Propositions as some types and universes}
So far, we've assumed that there is no difference between propositions and types. However, this view doesn't quite capture what a proposition is. To illustrate, let's define the existential quantifier, also known as a dependent pair:

\begin{definition}
  A sigma type/dependent pair is the type of pairs $(a, b(a))$, where the second entry is allowed to depend on the first:

\begin{lstlisting}
type P : A $\to$ Type $\vdash$ $\Sigma_{a:A} P(a)$
| intro$_{\Sigma}$ : $\Pi_{a:A}\, B(a) \to \Sigma_{a:A} B(a)$
\end{lstlisting}

  with derived induction rule:

\begin{lstlisting}
ind$_{\Sigma}$ : $(\Pi_{a:A} \Pi_{x:B(a)} P($intro$_\Sigma(a, x))) \to \Pi_{z : \Sigma_{a:A}\, B(a)}\, P(z)$
\end{lstlisting}
\end{definition}

It states that I can prove $\Sigma_{a:A}B(a)$ by exhibiting an element of type $A$ and a proof of $B(a)$. Thus it corresponds to $\exists a: B(a)$. At least in its construction. However, the induction rule is too strong. Indeed, we can construct projection functions:

\begin{theorem}
  The $\Sigma$ type has projection functions of the following type
\begin{lstlisting}
p$_{1}$ : $\Sigma_{a:A} B(a) \to A$
p$_{2}$ : $\Pi_{z : \Sigma_{a:A} B(a)} B(p_{1}(z))$
\end{lstlisting}
\end{theorem}
\begin{proof}
  We define the functions as follows:
\begin{lstlisting}
p$_{1}$(z) := ind$_{\Sigma}$($\la a \la x a$)
p$_{2}$(z) := ind$_{\Sigma}$($\la a \la x x$)
\end{lstlisting}
~
\end{proof}

The projection functions encode the axiom of choice. Given a proof of $\exists x: P(x)$, we can now pick an element $x$. This might be okay, if we only care about classical mathematics, but it would be good to have the option, whether or not to assume this axiom, instead of having it forced upon us.

The insight that solves this, is that a proposition doesn't have any ``content'', it is simply true or false. That is, once we have proved $\exists x: P(x)$, it should forget everything that went into the proof, and just remember the fact, that it is true. In other words, the type of a proposition should either be empty, or contain a single element.

This immediately tells us, that $\Sigma$ types are not propositions, since it potentially has many different elements. Disjunctions are also not propositions, since $\iota_{1}(a) \neq \iota_{2}(b)$. Natural number certainly aren't propositions, which is to expected, so what is a proposition? The types $\emptyset$ and $\mathbf 1$ are propositions, since they contain respectively 0 and 1 element. Also, if two types $A$ and $B$ are propositions, then the type $A \to B$ is a proposition, and $\Sigma_{a:A}B$ is also a proposition. In this case $\Sigma_{a:A}B$ is a conjunction, ``A and B.''

We can recover propositionality for existentials and disjunctions, by introducing \textit{universes}. Everything has a type, including types themselves. We used this to introduce \lstinline{nat_equals : $\N \to \N \to \Type$}. I claimed that $\N$, $\emptyset$ and $\mathbf 1$ all have type $\Type$. What is the type then, of $\Type$? Russels paradox still works in type theory, so we can't have $\Type : \Type$.

\begin{definition}
  A \textit{universe} is a type, which has types as its elements.
\end{definition}

In our type theory, we introduce a tower of universes, indexed by the natural numbers. To stay consistent with Lean, we call them \textit{sorts}.

\begin{definition}
  For each $n \in \N$ we have a universe $\Sort n$. We also have $\Sort n : Sort(1+n)$.

  We define two special universes
  \begin{align*}
    \Prop &:= \Sort 0\\
    \Type &:= \Sort 1
  \end{align*}

  As an axiom, we have that elements of $\Prop$ are propositions:
  \[prop_{\bullet} : \Pi_{P:\Prop}\Pi_{x, y:P}\, x=_{P}y\]
\end{definition}

$\Prop$ is the universe of propositions, and $\Type$ is the universe of regular types. The crucial this about universes, it that the induction principle of a type in universe $\Sort n$ can only produce types in that same universe. Thus, we can define the existential quantifier:

\begin{definition}
  The existential quantifier is a $\Sigma$ type in $\Prop$:
\begin{lstlisting}
type P : A $\to$ Prop $\vdash$ $\exists_{a:A} P(a)$ : Prop
| intro$_{\exists}$ : $\Pi_{a:A}\, P(a) \to \exists_{a:A}\, P(a)$
\end{lstlisting}
  with derived induction principle:
\begin{lstlisting}
ind$_{\exists}$ : $(\Pi_{a:A} \Pi_{x:B(a)}\, P($intro$_{\exists}(a, x))) \to \Pi_{z:\exists_{a:A}\,P(a)}\, P(z)$
\end{lstlisting}
\end{definition}
\textit{Remark.} We now have to annotate our type definitions with the universe they belong to. However, we haven't specified which universe $A$ belongs to. In that case, our definition is \textit{polymorphic} over universes, i.e. the definition applies for any $A : \Sort n$.

Now, since $P(a)$ is always a $\Prop$, we cannot define $p_{1}$ like we could for $\Sigma$ types. However, if we wish to prove a $\Prop$, we have access to $a:A$ using the induction principle.

We can define other propositions too:

\begin{definition}
  The order relation on the natural numbers is given by
\begin{lstlisting}
type n, m : $\N$ $\vdash$ $n \leq m$ : Prop
| zero_le : $\Pi_{n : \N}\, 0 \leq n$
| succ_le : $\Pi_{n, m : \N} (n \leq m) \to (succ(n) \leq succ(m))$
\end{lstlisting}
\end{definition}

\begin{definition}
  We have the type of a number being even:
\begin{lstlisting}
type n : $\N$ $\vdash$ even(n) : Prop
| zero_even : even(0)
| step_even : $\Pi_{n:\N}$ even(n) $\to$ even(succ(succ(n)))
\end{lstlisting}
\end{definition}

We don't have, in general, the law of excluded middle, i.e. $A \lor (A \to \emptyset)$. However, in some cases we can prove it:
\begin{definition}
  A proposition $A : \Prop$ for which we have $A \lor \neg A$ is called \textit{decidable}.
\end{definition}
% \begin{definition}
%   The type of booleans is defined asks
% \begin{lstlisting}
% type $\vdash$ $\B$
% | T : $\B$
% | F : $\B$
% \end{lstlisting}
% \end{definition}

% \begin{lemma}
%   Equality on $\mathbb B$ is decidable.
% \end{lemma}
% \begin{proof}~ We use the same technique as in theorem~\ref{thm:zero_ne_succ}:
% \begin{lstlisting}
% def $\B$_eq : $\B \to \B \to Prop$
% | T, T := $\mathbf 1$
% | T, F := $\emptyset$
% | F, T := $\emptyset$
% | F, F := $\mathbf 1$

% def $\B$_dec : $\Pi_{a, b : \B} (a=b) \lor \neg(a=b)$
% | T, T := $\iota_{1}(refl)$
% | T, F := $\iota_{2}$($\la h$ ind$_{=}$(T, $\bullet : \B$_eq$(T, T)$, F, h))
% | F, T := $\iota_{2}$($\la h$ ind$_{=}$(F, $\bullet : \B$_eq$(F, F)$, F, h))
% | F, F := $\iota_{1}(refl)$
% \end{lstlisting}
% \end{proof}




\begin{theorem}
  \lstinline{even(n)} is decidable.
\end{theorem}
\begin{proof}
  Similarly to the proof of theorem~\ref{thm:zero_ne_succ}, we construct a dependent type
\begin{lstlisting}
def is_even : $\N \to Prop$
| 0, h             := $\mathbf 1$
| succ(0), h       := $\emptyset$
| succ(succ(n)), h := is_even(n, h)

def ext : $\Pi_{n:\N}$ even(n) $\to$ is_even(n)
| 0, h       := $\bullet$
| succ(0), h := 

def step : $\Pi_{n:\N}$even(n) $\to$ is_even(n) $\to$ is_even(succ(succ(n)))
| n, h, b := b

even_dec : $\Pi_{n:\N}$ $\to$ even(n)$\lor$$\neg$even(n)
| 0 := $\iota_{1}$(zero_even)
| succ(0) := $\iota_2$($\la h$ ind$_{even}$(0, $\bullet$, succ(0), h))
| succ(succ(n)) := ind$_\lor$( $\iota_1(\la h$ step_even(n, h))
                       , $\iota_2(\la h$ )
                       , even_dec(n))
\end{lstlisting}
\end{proof}





\section{Mathematics in type theory and Lean}

\section{Gröbner bases as an extended example}

\end{document}
